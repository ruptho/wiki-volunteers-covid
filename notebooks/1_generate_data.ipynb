{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/jovyan/work')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.logger import Logger\n",
    "from helpers.vars import DUMPS_PATH, DATA_PATH, PRE_PATH, RES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load general Data and user kind, datetime, title normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.loadtxt(f'{DATA_PATH}/mediawiki_history_columns.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lang_history(lang, column_names, dtypes, path=DUMPS_PATH, ending='tsv.bz2', years=[2018, 2019, 2020]):\n",
    "    df_lang = pd.DataFrame()\n",
    "    # quick fix for small wikis\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_all = pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
    "        Logger.instance().info(f'Loaded {lang} in {time.time() - start}')\n",
    "        return df_all\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance().info(f'PROBABLY EXPECTED ERROR: No \"all-times\" file for {lang}')\n",
    "\n",
    "    for year in years:\n",
    "        start = time.time()\n",
    "        if lang == 'en':\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for month in range(1, 13): # just throw exception when out of bounds here\n",
    "                    df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}-{month:02d}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                    Logger.instance().info(f'Loaded {lang}-{year}-{month:02d} in {time.time() - start}')\n",
    "            except:\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}-{month}')\n",
    "        else:\n",
    "            try:\n",
    "                df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                Logger.instance().info(f'Loaded {lang}-{year} in {time.time() - start}')\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}')\n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import pendulum\n",
    "from mw.lib import title as mw_t\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en',\n",
       " 'sv',\n",
       " 'de',\n",
       " 'fr',\n",
       " 'nl',\n",
       " 'it',\n",
       " 'ja',\n",
       " 'ca',\n",
       " 'sr',\n",
       " 'no',\n",
       " 'ko',\n",
       " 'fi',\n",
       " 'da',\n",
       " 'tr']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_lang_codes = f'{DATA_PATH}/lang_code.csv'\n",
    "\n",
    "tz_ = dict(pd.read_csv(path_lang_codes).set_index(\"code\").timezone)\n",
    "tz = {code: pendulum.timezone(t) for code, t in tz_.items()}\n",
    "codes = list(tz.keys())\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PARAMS! (please ignore)\n",
    "#PRE_PATH = '../../data/coronawiki/testrun'\n",
    "#DUMPS_PATH = '../../data/coronawiki/mw-history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "for code in codes:\n",
    "    df_dict[code] = process_lang_history(code, column_names, path=DUMPS_PATH, dtypes=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{PRE_PATH}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # === user kind merged to hear\n",
    "        df_dict[code]['user_kind'] = df_dict[code].apply(lambda row: 'anonymous' if pd.isna(row.event_user_id) else 'bot' if not pd.isna(row.event_user_is_bot_by) else 'account', axis=1)\n",
    "        Logger.instance('pipeline').info(f'User-Kind assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Convert timestamp\n",
    "        df_dict[code]['event_timestamp_t'] = df_dict[code].event_timestamp.swifter.apply(pd.to_datetime) #pd.to_datetime(df_dict[code].event_timestamp)\n",
    "        # Set UTC date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_localize(\"UTC\", ambiguous='NaT', nonexistent='NaT')\n",
    "        df_dict[code][\"date_utc\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        # Localize date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_convert(tz_[code])\n",
    "        df_dict[code][\"date\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        Logger.instance('pipeline').info(f'Finished date assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Normalize date\n",
    "        df_dict[code]['page_title_norm'] = df_dict[code].page_title.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        df_dict[code]['page_title_historical_norm'] = df_dict[code].page_title_historical.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        Logger.instance('pipeline').info(f'Finished name normalization for {code} in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL! save preprocessing so far\n",
    "# NOTE: takes up lots of memory.\n",
    "# write to processed\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_dict[code].to_csv(f'{PRE_PATH}/{code}_mwh_processed.tsv.gz', header=True, index=False, sep=\"\\t\", compression='gzip')\n",
    "        Logger.instance('pipeline').info(f'Dumping {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error when saving {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and User_Kind to get newcomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.files import save_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 07:37 : INFO : User creation computation for en done in 912.7182765007019\n",
      "INFO:pipeline:User creation computation for en done in 912.7182765007019\n",
      "02-19 07:37 : INFO : User creation computation for sv done in 32.329793214797974\n",
      "INFO:pipeline:User creation computation for sv done in 32.329793214797974\n",
      "02-19 07:40 : INFO : User creation computation for de done in 136.2126841545105\n",
      "INFO:pipeline:User creation computation for de done in 136.2126841545105\n",
      "02-19 07:43 : INFO : User creation computation for fr done in 173.75721073150635\n",
      "INFO:pipeline:User creation computation for fr done in 173.75721073150635\n",
      "02-19 07:43 : INFO : User creation computation for nl done in 33.34605574607849\n",
      "INFO:pipeline:User creation computation for nl done in 33.34605574607849\n",
      "02-19 07:45 : INFO : User creation computation for it done in 106.61678528785706\n",
      "INFO:pipeline:User creation computation for it done in 106.61678528785706\n",
      "02-19 07:46 : INFO : User creation computation for ja done in 60.482234716415405\n",
      "INFO:pipeline:User creation computation for ja done in 60.482234716415405\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-81e9f380c637>\", line 7, in <module>\n",
      "    create_event_mask = (df_code.event_entity=='user') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 07:46 : INFO : Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 07:47 : INFO : User creation computation for sr done in 43.35005593299866\n",
      "INFO:pipeline:User creation computation for sr done in 43.35005593299866\n",
      "02-19 07:47 : INFO : User creation computation for no done in 12.92446231842041\n",
      "INFO:pipeline:User creation computation for no done in 12.92446231842041\n",
      "02-19 07:48 : INFO : User creation computation for ko done in 36.90428018569946\n",
      "INFO:pipeline:User creation computation for ko done in 36.90428018569946\n",
      "02-19 07:48 : INFO : User creation computation for fi done in 10.674769878387451\n",
      "INFO:pipeline:User creation computation for fi done in 10.674769878387451\n",
      "02-19 07:48 : INFO : User creation computation for da done in 5.546395778656006\n",
      "INFO:pipeline:User creation computation for da done in 5.546395778656006\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-13-81e9f380c637>\", line 7, in <module>\n",
      "    create_event_mask = (df_code.event_entity=='user') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 07:48 : INFO : Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 07:48 : INFO : Finished newcomers\n",
      "INFO:pipeline:Finished newcomers\n"
     ]
    }
   ],
   "source": [
    "dict_creation = {}\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        df_code = df_dict[code]\n",
    "        # define masks\n",
    "        create_event_mask = (df_code.event_entity=='user') & (df_code.event_type == 'create')\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        no_bot_mask = (df_code['event_user_is_bot_by'].isna() | df_code['event_user_is_bot_by_historical'].isna())\n",
    "        self_creation_mask = (df_code['event_user_is_created_by_self'] == 'true')\n",
    "        no_anon_mask = (df_code['event_user_is_anonymous'] != 'true')\n",
    "\n",
    "        # === get users by registration\n",
    "        group_creation = df_code[create_event_mask & no_anon_mask & no_bot_mask & self_creation_mask].groupby(['date'])['event_user_id'].size()\n",
    "\n",
    "        # === get user by nth edit\n",
    "        # n=1\n",
    "        edit_count_mask = df_code.event_user_revision_count == '1'\n",
    "        group_edit1 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_1')\n",
    "        # n=5\n",
    "        edit_count_mask = df_code.event_user_revision_count == '5'\n",
    "        group_edit5 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_5')\n",
    "        dict_creation[code] = pd.concat([group_creation, group_edit1, group_edit5], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'User creation computation for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')\n",
    "        \n",
    "save_to_pickle(f'{PRE_PATH}/dict_newcomers_selfcreated.pkl', dict_creation)\n",
    "Logger.instance('pipeline').info(f'Finished newcomers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and Page and user_kind to get edits per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 09:02 : INFO : Grouped by user/user_kind/pageid for en done in 4440.065267801285\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for en done in 4440.065267801285\n",
      "02-19 09:10 : INFO : Grouped by user/user_kind/pageid for sv done in 468.8935797214508\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for sv done in 468.8935797214508\n",
      "02-19 09:22 : INFO : Grouped by user/user_kind/pageid for de done in 723.7860300540924\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for de done in 723.7860300540924\n",
      "02-19 09:37 : INFO : Grouped by user/user_kind/pageid for fr done in 893.5918982028961\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for fr done in 893.5918982028961\n",
      "02-19 09:40 : INFO : Grouped by user/user_kind/pageid for nl done in 185.4799027442932\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for nl done in 185.4799027442932\n",
      "02-19 09:52 : INFO : Grouped by user/user_kind/pageid for it done in 763.2101054191589\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for it done in 763.2101054191589\n",
      "02-19 09:59 : INFO : Grouped by user/user_kind/pageid for ja done in 393.74922609329224\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for ja done in 393.74922609329224\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-14-48c00f35c0d4>\", line 8, in <module>\n",
      "    create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 09:59 : INFO : Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:04 : INFO : Grouped by user/user_kind/pageid for sr done in 301.20770168304443\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for sr done in 301.20770168304443\n",
      "02-19 10:05 : INFO : Grouped by user/user_kind/pageid for no done in 76.28237390518188\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for no done in 76.28237390518188\n",
      "02-19 10:08 : INFO : Grouped by user/user_kind/pageid for ko done in 186.81720304489136\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for ko done in 186.81720304489136\n",
      "02-19 10:09 : INFO : Grouped by user/user_kind/pageid for fi done in 62.2643666267395\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for fi done in 62.2643666267395\n",
      "02-19 10:10 : INFO : Grouped by user/user_kind/pageid for da done in 30.636804819107056\n",
      "INFO:pipeline:Grouped by user/user_kind/pageid for da done in 30.636804819107056\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-14-48c00f35c0d4>\", line 8, in <module>\n",
      "    create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:10 : INFO : Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:37 : INFO : Finished edits\n",
      "INFO:pipeline:Finished edits\n"
     ]
    }
   ],
   "source": [
    "# Group By Date and Page and user_kind to get edits per day\n",
    "dict_edits_byid = {} # grouped by id\n",
    "dict_edits_bytitle = {} # grouped by title\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # group by date, page_id, user_kind\n",
    "        df_code.revision_text_bytes_diff = pd.to_numeric(df_code['revision_text_bytes_diff'], errors='coerce').fillna(0)\n",
    "        df_code_masked = df_code[create_revision_mask & ns_mask]\n",
    "        \n",
    "        dict_edits_byid[code] = df_code_masked.groupby(['date', 'page_id', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_title': 'last', \n",
    "             'page_title_norm': 'last', 'page_title_historical_norm': 'last'})\n",
    "        dict_edits_bytitle[code] = df_code_masked.groupby(['date', 'page_title_norm', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_id': lambda x: set(x), 'page_title': lambda x: set(x),  'page_title_historical_norm': lambda x: set(x)})\n",
    "\n",
    "        Logger.instance('pipeline').info(f'Grouped by user/user_kind/pageid for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_byid.pkl', dict_edits_byid)\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle.pkl', dict_edits_bytitle)\n",
    "Logger.instance('pipeline').info(f'Finished edits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and user_kind to get identity reverts per day (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 10:43 : INFO : Computed reverts by en done in 383.1931080818176\n",
      "INFO:pipeline:Computed reverts by en done in 383.1931080818176\n",
      "02-19 10:43 : INFO : Computed reverts by sv done in 8.665184497833252\n",
      "INFO:pipeline:Computed reverts by sv done in 8.665184497833252\n",
      "02-19 10:44 : INFO : Computed reverts by de done in 51.96122360229492\n",
      "INFO:pipeline:Computed reverts by de done in 51.96122360229492\n",
      "02-19 10:45 : INFO : Computed reverts by fr done in 52.19099450111389\n",
      "INFO:pipeline:Computed reverts by fr done in 52.19099450111389\n",
      "02-19 10:45 : INFO : Computed reverts by nl done in 13.434366464614868\n",
      "INFO:pipeline:Computed reverts by nl done in 13.434366464614868\n",
      "02-19 10:46 : INFO : Computed reverts by it done in 41.403966426849365\n",
      "INFO:pipeline:Computed reverts by it done in 41.403966426849365\n",
      "02-19 10:46 : INFO : Computed reverts by ja done in 27.565186738967896\n",
      "INFO:pipeline:Computed reverts by ja done in 27.565186738967896\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-13a30b5a5397>\", line 6, in <module>\n",
      "    create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:46 : INFO : Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for ca: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:47 : INFO : Computed reverts by sr done in 9.09570837020874\n",
      "INFO:pipeline:Computed reverts by sr done in 9.09570837020874\n",
      "02-19 10:47 : INFO : Computed reverts by no done in 5.281750917434692\n",
      "INFO:pipeline:Computed reverts by no done in 5.281750917434692\n",
      "02-19 10:47 : INFO : Computed reverts by ko done in 16.017684936523438\n",
      "INFO:pipeline:Computed reverts by ko done in 16.017684936523438\n",
      "02-19 10:47 : INFO : Computed reverts by fi done in 4.706100940704346\n",
      "INFO:pipeline:Computed reverts by fi done in 4.706100940704346\n",
      "02-19 10:47 : INFO : Computed reverts by da done in 2.0776994228363037\n",
      "INFO:pipeline:Computed reverts by da done in 2.0776994228363037\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-13a30b5a5397>\", line 6, in <module>\n",
      "    create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\", line 5274, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:47 : INFO : Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "INFO:pipeline:Error for tr: 'DataFrame' object has no attribute 'event_entity'\n",
      "02-19 10:47 : INFO : Finished identity reverts\n",
      "INFO:pipeline:Finished identity reverts\n"
     ]
    }
   ],
   "source": [
    "dict_reverts = {}\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # get reverts per day as well as reverted\n",
    "        df_reverted = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_reverted == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_reverted'].size()\n",
    "        df_reverts = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_revert == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_revert'].size()\n",
    "        \n",
    "        # reindex so all dates are filled\n",
    "        df_reverted = df_reverted.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverted.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "        df_reverts = df_reverts.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverts.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "    \n",
    "        dict_reverts[code] = pd.concat([df_reverted, df_reverts], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'Computed reverts by {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_reverts.pkl', dict_reverts)\n",
    "Logger.instance('pipeline').info(f'Finished identity reverts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine edit dictionary with covid info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file was pre-generated using the .json-list of COVID-articles from:\n",
    "# https://covid-data.wmflabs.org/\n",
    "path_covid = f'{DATA_PATH}/covid_linked.f'\n",
    "df_covid = pd.read_feather(path_covid)\n",
    "df_covid['covid'] = True\n",
    "df_covid['index'] = df_covid['index'].apply(lambda t: mw_t.normalize(str(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edits_covid = {}\n",
    "for code, df_code in dict_edits_bytitle.items():\n",
    "    df_edits_covid[code] = df_code.reset_index().merge(df_covid[df_covid.site == f'{code}wiki'], left_on=['page_title_norm'], right_on=['index'], how='left').fillna({'covid': False}).drop(['index', 'site', 'qid'], axis=1)\n",
    "\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle_covid.pkl', df_edits_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Final Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing import aggregate_preprocess_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:21 : INFO : Processing en took 400.5793981552124\n",
      "INFO:pipeline:Processing en took 400.5793981552124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:21 : INFO : Processing sv took 25.778750896453857\n",
      "INFO:pipeline:Processing sv took 25.778750896453857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:23 : INFO : Processing de took 74.87158846855164\n",
      "INFO:pipeline:Processing de took 74.87158846855164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:24 : INFO : Processing fr took 74.31901431083679\n",
      "INFO:pipeline:Processing fr took 74.31901431083679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:24 : INFO : Processing nl took 17.01649022102356\n",
      "INFO:pipeline:Processing nl took 17.01649022102356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:25 : INFO : Processing it took 57.786693811416626\n",
      "INFO:pipeline:Processing it took 57.786693811416626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:26 : INFO : Processing ja took 35.69470715522766\n",
      "INFO:pipeline:Processing ja took 35.69470715522766\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/work/helpers/preprocessing.py\", line 42, in aggregate_preprocess_results\n",
      "    df_gb = process_edits(dict_edits, code)\n",
      "  File \"/home/jovyan/work/helpers/preprocessing.py\", line 10, in process_edits\n",
      "    df_code = dict_date[code].reset_index()\n",
      "KeyError: 'ca'\n",
      "02-19 11:26 : INFO : Error for ca: 'ca'\n",
      "INFO:pipeline:Error for ca: 'ca'\n",
      "02-19 11:26 : INFO : Processing ca took 0.002092599868774414\n",
      "INFO:pipeline:Processing ca took 0.002092599868774414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n",
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:26 : INFO : Processing sr took 36.63195300102234\n",
      "INFO:pipeline:Processing sr took 36.63195300102234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:26 : INFO : Processing no took 6.5973920822143555\n",
      "INFO:pipeline:Processing no took 6.5973920822143555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:27 : INFO : Processing ko took 16.5759117603302\n",
      "INFO:pipeline:Processing ko took 16.5759117603302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:27 : INFO : Processing fi took 5.3210015296936035\n",
      "INFO:pipeline:Processing fi took 5.3210015296936035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-19 11:27 : INFO : Processing da took 2.4243338108062744\n",
      "INFO:pipeline:Processing da took 2.4243338108062744\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/work/helpers/preprocessing.py\", line 42, in aggregate_preprocess_results\n",
      "    df_gb = process_edits(dict_edits, code)\n",
      "  File \"/home/jovyan/work/helpers/preprocessing.py\", line 10, in process_edits\n",
      "    df_code = dict_date[code].reset_index()\n",
      "KeyError: 'tr'\n",
      "02-19 11:27 : INFO : Error for tr: 'tr'\n",
      "INFO:pipeline:Error for tr: 'tr'\n",
      "02-19 11:27 : INFO : Processing tr took 0.0016481876373291016\n",
      "INFO:pipeline:Processing tr took 0.0016481876373291016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>covid</th>\n",
       "      <th>user_kind</th>\n",
       "      <th>count</th>\n",
       "      <th>rev_len_sum</th>\n",
       "      <th>actor_user</th>\n",
       "      <th>edit_1</th>\n",
       "      <th>edit_5</th>\n",
       "      <th>revision_is_identity_reverted</th>\n",
       "      <th>revision_is_identity_revert</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>71799</td>\n",
       "      <td>1.929916e+09</td>\n",
       "      <td>4178.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>6011</td>\n",
       "      <td>6683</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>23740</td>\n",
       "      <td>7.026144e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7155</td>\n",
       "      <td>1083</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>7196</td>\n",
       "      <td>1.487072e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>257</td>\n",
       "      <td>627</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>account</td>\n",
       "      <td>4</td>\n",
       "      <td>1.942120e+05</td>\n",
       "      <td>4178.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>6011</td>\n",
       "      <td>6683</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>81001</td>\n",
       "      <td>1.992566e+09</td>\n",
       "      <td>5155.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>6355</td>\n",
       "      <td>7728</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>True</td>\n",
       "      <td>account</td>\n",
       "      <td>1</td>\n",
       "      <td>1.020430e+05</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>307</td>\n",
       "      <td>3.912818e+06</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>134</td>\n",
       "      <td>1.528426e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>2</td>\n",
       "      <td>1.804700e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>True</td>\n",
       "      <td>account</td>\n",
       "      <td>1</td>\n",
       "      <td>1.112300e+04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45922 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  covid  user_kind  count   rev_len_sum  actor_user  edit_1  \\\n",
       "0    2018-01-01  False    account  71799  1.929916e+09      4178.0  1297.0   \n",
       "1    2018-01-01  False  anonymous  23740  7.026144e+08         0.0     0.0   \n",
       "2    2018-01-01  False        bot   7196  1.487072e+08         0.0     0.0   \n",
       "3    2018-01-01   True    account      4  1.942120e+05      4178.0  1297.0   \n",
       "4    2018-01-02  False    account  81001  1.992566e+09      5155.0  1905.0   \n",
       "...         ...    ...        ...    ...           ...         ...     ...   \n",
       "3423 2020-11-30   True    account      1  1.020430e+05        16.0    12.0   \n",
       "3424 2020-12-01  False    account    307  3.912818e+06        10.0    12.0   \n",
       "3425 2020-12-01  False  anonymous    134  1.528426e+06         0.0     0.0   \n",
       "3426 2020-12-01  False        bot      2  1.804700e+04         0.0     0.0   \n",
       "3427 2020-12-01   True    account      1  1.112300e+04        10.0    12.0   \n",
       "\n",
       "      edit_5  revision_is_identity_reverted  revision_is_identity_revert code  \n",
       "0      348.0                           6011                         6683   en  \n",
       "1        0.0                           7155                         1083   en  \n",
       "2        0.0                            257                          627   en  \n",
       "3      348.0                           6011                         6683   en  \n",
       "4      485.0                           6355                         7728   en  \n",
       "...      ...                            ...                          ...  ...  \n",
       "3423     2.0                              5                           29   da  \n",
       "3424     2.0                             19                           35   da  \n",
       "3425     0.0                             44                           11   da  \n",
       "3426     0.0                              0                            0   da  \n",
       "3427     2.0                             19                           35   da  \n",
       "\n",
       "[45922 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_agg = aggregate_preprocess_results(codes, df_edits_covid, dict_creation, dict_reverts)\n",
    "final_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{RES_PATH}').mkdir(parents=True, exist_ok=True)\n",
    "final_agg.to_csv(f'{RES_PATH}/aggregated.tsv.gz', index=False, sep=\"\\t\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}