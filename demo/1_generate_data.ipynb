{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/jovyan/work')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.logger import Logger\n",
    "from helpers.vars import DUMPS_PATH, DATA_PATH, PRE_PATH, RES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load general Data and user kind, datetime, title normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.loadtxt(f'{DATA_PATH}/mediawiki_history_columns.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lang_history(lang, column_names, dtypes, path=DUMPS_PATH, ending='tsv.bz2', years=[2018, 2019, 2020]):\n",
    "    df_lang = pd.DataFrame()\n",
    "    # quick fix for small wikis\n",
    "    try:\n",
    "        return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
    "        Logger.instance().info(f'Loaded {lang} in {time.time() - start}')\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance().info(f'PROBABLY EXPECTED ERROR: No \"all-times\" file for {lang}')\n",
    "                    \n",
    "    for year in years:\n",
    "        start = time.time()\n",
    "        if lang == 'en':\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for month in range(1, 13): # just throw exception when out of bounds here\n",
    "                    df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}-{month:02d}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                    Logger.instance().info(f'Loaded {lang}-{year}-{month:02d} in {time.time() - start}')\n",
    "            except:\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}-{month}')\n",
    "        else:\n",
    "            try:\n",
    "                df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                Logger.instance().info(f'Loaded {lang}-{year} in {time.time() - start}')\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}')\n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import pendulum\n",
    "from mw.lib import title as mw_t\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pl', 'cs', 'uk', 'ru']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we manually set the timezones here instead of using our pre-loading code\n",
    "# path_lang_codes = f'{DATA_PATH}/lang_code.csv'\n",
    "#tz_ = dict(pd.read_csv(path_lang_codes).set_index(\"code\").timezone)\n",
    "tz_ = {'pl': 'Europe/Warsaw',\n",
    "'cs': 'Europe/Prague',\n",
    "'uk': 'Europe/Minsk',\n",
    "'ru': 'Europe/Moscow'}\n",
    "tz = {code: pendulum.timezone(t) for code, t in tz_.items()}\n",
    "codes = list(tz.keys())\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/preprocessing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/pl.tsv.bz2'\n",
      "09-01 09:15 : INFO : Created new singleton instance\n",
      "09-01 09:15 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for pl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logging instance for ../logging//coronawiki.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 09:17 : INFO : Loaded pl-2018 in 125.28399729728699\n",
      "09-01 09:19 : INFO : Loaded pl-2019 in 103.07519245147705\n",
      "09-01 09:21 : INFO : Loaded pl-2020 in 126.96920680999756\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/cs.tsv.bz2'\n",
      "09-01 09:21 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for cs\n",
      "09-01 09:21 : INFO : Loaded cs-2018 in 37.194085121154785\n",
      "09-01 09:22 : INFO : Loaded cs-2019 in 41.158761978149414\n",
      "09-01 09:23 : INFO : Loaded cs-2020 in 49.66367030143738\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/uk.tsv.bz2'\n",
      "09-01 09:23 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for uk\n",
      "09-01 09:24 : INFO : Loaded uk-2018 in 83.37940955162048\n",
      "09-01 09:26 : INFO : Loaded uk-2019 in 103.62503457069397\n",
      "09-01 09:28 : INFO : Loaded uk-2020 in 140.5416386127472\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/ru.tsv.bz2'\n",
      "09-01 09:28 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for ru\n",
      "09-01 09:33 : INFO : Loaded ru-2018 in 257.1337504386902\n",
      "09-01 09:37 : INFO : Loaded ru-2019 in 271.5057547092438\n",
      "09-01 09:42 : INFO : Loaded ru-2020 in 286.01929116249084\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "for code in codes:\n",
    "    df_dict[code] = process_lang_history(code, column_names, path=DUMPS_PATH, dtypes=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 09:47 : INFO : Created new singleton instance\n",
      "09-01 09:47 : INFO : User-Kind assignment for pl in 291.5884621143341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logging instance for ../logging//pipeline.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "09-01 09:51 : INFO : Finished date assignment for pl in 520.4055099487305\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad22e54277eb4366b13cf4e63d263b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12279549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c963821dee41639296406dc00737eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12279549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 09:52 : INFO : Finished name normalization for pl in 616.0669770240784\n",
      "09-01 09:54 : INFO : User-Kind assignment for cs in 103.18351817131042\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "09-01 09:55 : INFO : Finished date assignment for cs in 187.84202647209167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b29fa93e2f43b58d677faa269b25c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4357796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9162a8e6dea24351be6e8a7861891fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4357796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 09:56 : INFO : Finished name normalization for cs in 221.7209756374359\n",
      "09-01 10:01 : INFO : User-Kind assignment for uk in 285.04049277305603\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "09-01 10:04 : INFO : Finished date assignment for uk in 505.7473430633545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfe4050216c48809c1b7e7e28c7f7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11808705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e54cdfc21248368bd5775186adea8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11808705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:06 : INFO : Finished name normalization for uk in 599.4014620780945\n",
      "09-01 10:16 : INFO : User-Kind assignment for ru in 583.876383304596\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "09-01 10:24 : INFO : Finished date assignment for ru in 1064.6063442230225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eaa23f0118458fb8ba34934a5e9f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25160718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dcaff0b8ca4e468c37bd5521a9fe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25160718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:27 : INFO : Finished name normalization for ru in 1265.944834470749\n"
     ]
    }
   ],
   "source": [
    "Path(f'{PRE_PATH}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # === user kind merged to hear\n",
    "        df_dict[code]['user_kind'] = df_dict[code].apply(lambda row: 'anonymous' if pd.isna(row.event_user_id) else 'bot' if not pd.isna(row.event_user_is_bot_by) else 'account', axis=1)\n",
    "        Logger.instance('pipeline').info(f'User-Kind assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Convert timestamp\n",
    "        df_dict[code]['event_timestamp_t'] = df_dict[code].event_timestamp.swifter.apply(pd.to_datetime) #pd.to_datetime(df_dict[code].event_timestamp)\n",
    "        # Set UTC date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_localize(\"UTC\", ambiguous='NaT', nonexistent='NaT')\n",
    "        df_dict[code][\"date_utc\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        # Localize date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_convert(tz_[code])\n",
    "        df_dict[code][\"date\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        Logger.instance('pipeline').info(f'Finished date assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Normalize date\n",
    "        df_dict[code]['page_title_norm'] = df_dict[code].page_title.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        df_dict[code]['page_title_historical_norm'] = df_dict[code].page_title_historical.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        Logger.instance('pipeline').info(f'Finished name normalization for {code} in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL! save preprocessing so far\n",
    "# NOTE: takes up lots of memory.\n",
    "# write to processed\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_dict[code].to_csv(f'{PRE_PATH}/{code}_mwh_processed.tsv.gz', header=True, index=False, sep=\"\\t\", compression='gzip')\n",
    "        Logger.instance('pipeline').info(f'Dumping {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error when saving {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and User_Kind to get newcomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.files import save_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:28 : INFO : User creation computation for pl done in 43.03811717033386\n",
      "09-01 10:28 : INFO : User creation computation for cs done in 13.797757148742676\n",
      "09-01 10:28 : INFO : User creation computation for uk done in 36.32571029663086\n",
      "09-01 10:30 : INFO : User creation computation for ru done in 89.68020296096802\n",
      "09-01 10:30 : INFO : Finished newcomers\n"
     ]
    }
   ],
   "source": [
    "dict_creation = {}\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        df_code = df_dict[code]\n",
    "        # define masks\n",
    "        create_event_mask = (df_code.event_entity=='user') & (df_code.event_type == 'create')\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        no_bot_mask = (df_code['event_user_is_bot_by'].isna() | df_code['event_user_is_bot_by_historical'].isna())\n",
    "        self_creation_mask = (df_code['event_user_is_created_by_self'] == 'true')\n",
    "        no_anon_mask = (df_code['event_user_is_anonymous'] != 'true')\n",
    "\n",
    "        # === get users by registration\n",
    "        group_creation = df_code[create_event_mask & no_anon_mask & no_bot_mask & self_creation_mask].groupby(['date'])['event_user_id'].size()\n",
    "\n",
    "        # === get user by nth edit\n",
    "        # n=1\n",
    "        edit_count_mask = df_code.event_user_revision_count == '1'\n",
    "        group_edit1 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_1')\n",
    "        # n=5\n",
    "        edit_count_mask = df_code.event_user_revision_count == '5'\n",
    "        group_edit5 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_5')\n",
    "        dict_creation[code] = pd.concat([group_creation, group_edit1, group_edit5], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'User creation computation for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')\n",
    "        \n",
    "save_to_pickle(f'{PRE_PATH}/dict_newcomers_selfcreated_demo.pkl', dict_creation)\n",
    "Logger.instance('pipeline').info(f'Finished newcomers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and Page and user_kind to get edits per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:36 : INFO : Grouped by user/user_kind/pageid for pl done in 387.51079082489014\n",
      "09-01 10:39 : INFO : Grouped by user/user_kind/pageid for cs done in 128.68407654762268\n",
      "09-01 10:43 : INFO : Grouped by user/user_kind/pageid for uk done in 275.36093950271606\n",
      "09-01 10:54 : INFO : Grouped by user/user_kind/pageid for ru done in 640.466635465622\n",
      "09-01 10:56 : INFO : Finished edits\n"
     ]
    }
   ],
   "source": [
    "# Group By Date and Page and user_kind to get edits per day\n",
    "dict_edits_byid = {} # grouped by id\n",
    "dict_edits_bytitle = {} # grouped by title\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # group by date, page_id, user_kind\n",
    "        df_code.revision_text_bytes_diff = pd.to_numeric(df_code['revision_text_bytes_diff'], errors='coerce').fillna(0)\n",
    "        df_code_masked = df_code[create_revision_mask & ns_mask]\n",
    "        \n",
    "        dict_edits_byid[code] = df_code_masked.groupby(['date', 'page_id', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_title': 'last', \n",
    "             'page_title_norm': 'last', 'page_title_historical_norm': 'last'})\n",
    "        dict_edits_bytitle[code] = df_code_masked.groupby(['date', 'page_title_norm', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_id': lambda x: set(x), 'page_title': lambda x: set(x),  'page_title_historical_norm': lambda x: set(x)})\n",
    "\n",
    "        Logger.instance('pipeline').info(f'Grouped by user/user_kind/pageid for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_byid_demo.pkl', dict_edits_byid)\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle_demo.pkl', dict_edits_bytitle)\n",
    "Logger.instance('pipeline').info(f'Finished edits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and user_kind to get identity reverts per day (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:56 : INFO : Computed reverts by pl done in 13.750905275344849\n",
      "09-01 10:56 : INFO : Computed reverts by cs done in 4.97031044960022\n",
      "09-01 10:56 : INFO : Computed reverts by uk done in 11.530215978622437\n",
      "09-01 10:57 : INFO : Computed reverts by ru done in 34.912052154541016\n",
      "09-01 10:57 : INFO : Finished identity reverts\n"
     ]
    }
   ],
   "source": [
    "dict_reverts = {}\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # get reverts per day as well as reverted\n",
    "        df_reverted = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_reverted == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_reverted'].size()\n",
    "        df_reverts = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_revert == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_revert'].size()\n",
    "        \n",
    "        # reindex so all dates are filled\n",
    "        df_reverted = df_reverted.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverted.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "        df_reverts = df_reverts.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverts.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "    \n",
    "        dict_reverts[code] = pd.concat([df_reverted, df_reverts], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'Computed reverts by {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_reverts_demo.pkl', dict_reverts)\n",
    "Logger.instance('pipeline').info(f'Finished identity reverts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine edit dictionary with covid info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple demo, we exclude covid analysis. However, one could easily declare the articles they want to consider as COVID-19 articles (e.g., retrieving from https://covid-data.wmflabs.org/ or crawling COVID-categories via https://petscan.wmflabs.org/) and merge the result with the given dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file was pre-generated using the .json-list of COVID-articles from:\n",
    "# https://covid-data.wmflabs.org/\n",
    "'''\n",
    "path_covid = f'{DATA_PATH}/covid_linked.f'\n",
    "df_covid = pd.read_feather(path_covid)\n",
    "df_covid['covid'] = True\n",
    "df_covid['index'] = df_covid['index'].apply(lambda t: mw_t.normalize(str(t)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_edits_covid = {}\n",
    "for code, df_code in dict_edits_bytitle.items():\n",
    "    df_edits_covid[code] = df_code.reset_index().merge(df_covid[df_covid.site == f'{code}wiki'], left_on=['page_title_norm'], right_on=['index'], how='left').fillna({'covid': False}).drop(['index', 'site', 'qid'], axis=1)\n",
    "\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle_covid.pkl', df_edits_covid)\n",
    "'''\n",
    "for code, df_code in dict_edits_bytitle.items():\n",
    "    df_code['covid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Final Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing import aggregate_preprocess_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:57 : INFO : Processing pl took 23.426910638809204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:58 : INFO : Processing cs took 7.83440637588501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:58 : INFO : Processing uk took 17.97576141357422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-01 10:59 : INFO : Processing ru took 40.75509834289551\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>covid</th>\n",
       "      <th>user_kind</th>\n",
       "      <th>count</th>\n",
       "      <th>rev_len_sum</th>\n",
       "      <th>actor_user</th>\n",
       "      <th>edit_1</th>\n",
       "      <th>edit_5</th>\n",
       "      <th>revision_is_identity_reverted</th>\n",
       "      <th>revision_is_identity_revert</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>4067</td>\n",
       "      <td>1188498.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>134</td>\n",
       "      <td>304</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>629</td>\n",
       "      <td>132469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>25</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>560</td>\n",
       "      <td>7606.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>4264</td>\n",
       "      <td>1102137.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>143</td>\n",
       "      <td>314</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>824</td>\n",
       "      <td>87568.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331</td>\n",
       "      <td>18</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>2377</td>\n",
       "      <td>186786.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540</td>\n",
       "      <td>85</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3287</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>1254</td>\n",
       "      <td>342755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>1078</td>\n",
       "      <td>472596.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>206</td>\n",
       "      <td>21604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>127</td>\n",
       "      <td>18464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13162 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  covid  user_kind  count  rev_len_sum  actor_user  edit_1  \\\n",
       "0    2018-01-01  False    account   4067    1188498.0        54.0    20.0   \n",
       "1    2018-01-01  False  anonymous    629     132469.0         0.0     0.0   \n",
       "2    2018-01-01  False        bot    560       7606.0         0.0     0.0   \n",
       "3    2018-01-02  False    account   4264    1102137.0       168.0    76.0   \n",
       "4    2018-01-02  False  anonymous    824      87568.0         0.0     0.0   \n",
       "...         ...    ...        ...    ...          ...         ...     ...   \n",
       "3286 2020-12-31  False  anonymous   2377     186786.0         0.0     0.0   \n",
       "3287 2020-12-31  False        bot   1254     342755.0         0.0     0.0   \n",
       "3288 2021-01-01  False    account   1078     472596.0        15.0     8.0   \n",
       "3289 2021-01-01  False  anonymous    206      21604.0         0.0     0.0   \n",
       "3290 2021-01-01  False        bot    127      18464.0         0.0     0.0   \n",
       "\n",
       "      edit_5  revision_is_identity_reverted  revision_is_identity_revert code  \n",
       "0       16.0                            134                          304   pl  \n",
       "1        0.0                            219                           25   pl  \n",
       "2        0.0                              5                            2   pl  \n",
       "3       25.0                            143                          314   pl  \n",
       "4        0.0                            331                           18   pl  \n",
       "...      ...                            ...                          ...  ...  \n",
       "3286     0.0                            540                           85   ru  \n",
       "3287     0.0                             20                           52   ru  \n",
       "3288     4.0                             40                           48   ru  \n",
       "3289     0.0                             65                            5   ru  \n",
       "3290     0.0                              0                           16   ru  \n",
       "\n",
       "[13162 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_agg = aggregate_preprocess_results(codes, df_edits_covid, dict_creation, dict_reverts)\n",
    "final_agg = aggregate_preprocess_results(codes, dict_edits_bytitle, dict_creation, dict_reverts)\n",
    "final_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{RES_PATH}/demo/').mkdir(parents=True, exist_ok=True)\n",
    "final_agg.to_csv(f'{RES_PATH}/demo/aggregated_demo.tsv.gz', index=False, sep=\"\\t\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
