{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/jovyan/work')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.logger import Logger\n",
    "from helpers.vars import DUMPS_PATH, DATA_PATH, PRE_PATH, RES_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load general Data and user kind, datetime, title normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.loadtxt(f'{DATA_PATH}/mediawiki_history_columns.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lang_history(lang, column_names, dtypes, path=DUMPS_PATH, ending='tsv.bz2', years=[2018, 2019, 2020]):\n",
    "    df_lang = pd.DataFrame()\n",
    "    # quick fix for small wikis\n",
    "    try:\n",
    "        return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
    "        Logger.instance().info(f'Loaded {lang} in {time.time() - start}')\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance().info(f'PROBABLY EXPECTED ERROR: No \"all-times\" file for {lang}')\n",
    "                    \n",
    "    for year in years:\n",
    "        start = time.time()\n",
    "        if lang == 'en':\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for month in range(1, 13): # just throw exception when out of bounds here\n",
    "                    df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}-{month:02d}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                    Logger.instance().info(f'Loaded {lang}-{year}-{month:02d} in {time.time() - start}')\n",
    "            except:\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}-{month}')\n",
    "        else:\n",
    "            try:\n",
    "                df_lang = pd.concat([df_lang, pd.read_csv(f'{path}/{lang}-{year}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)])\n",
    "                Logger.instance().info(f'Loaded {lang}-{year} in {time.time() - start}')\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                Logger.instance().info(f'Error when processing {lang}-{year}')\n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import pendulum\n",
    "from mw.lib import title as mw_t\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pl', 'cs', 'uk', 'ru']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we manually set the timezones here instead of using our pre-loading code\n",
    "# path_lang_codes = f'{DATA_PATH}/lang_code.csv'\n",
    "#tz_ = dict(pd.read_csv(path_lang_codes).set_index(\"code\").timezone)\n",
    "tz_ = {'pl': 'Europe/Warsaw',\n",
    "'cs': 'Europe/Prague',\n",
    "'uk': 'Europe/Minsk',\n",
    "'ru': 'Europe/Moscow'}\n",
    "tz = {code: pendulum.timezone(t) for code, t in tz_.items()}\n",
    "codes = list(tz.keys())\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/preprocessing'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/pl.tsv.bz2'\n",
      "08-20 09:37 : INFO : Created new singleton instance\n",
      "08-20 09:37 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for pl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logging instance for ../logging//coronawiki.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 09:40 : INFO : Loaded pl-2018 in 135.09784245491028\n",
      "08-20 09:41 : INFO : Loaded pl-2019 in 113.83451199531555\n",
      "08-20 09:44 : INFO : Loaded pl-2020 in 141.7149477005005\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/cs.tsv.bz2'\n",
      "08-20 09:44 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for cs\n",
      "08-20 09:44 : INFO : Loaded cs-2018 in 41.277350664138794\n",
      "08-20 09:45 : INFO : Loaded cs-2019 in 47.13198471069336\n",
      "08-20 09:46 : INFO : Loaded cs-2020 in 51.896336793899536\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/uk.tsv.bz2'\n",
      "08-20 09:46 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for uk\n",
      "08-20 09:48 : INFO : Loaded uk-2018 in 88.69151496887207\n",
      "08-20 09:49 : INFO : Loaded uk-2019 in 108.03377103805542\n",
      "08-20 09:52 : INFO : Loaded uk-2020 in 146.21737670898438\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c1aa7e8a0571>\", line 5, in process_lang_history\n",
      "    return pd.read_csv(f'{path}/{lang}.{ending}', sep='\\t', names=list(column_names), dtype=dtypes, warn_bad_lines=True, error_bad_lines=False)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 462, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 819, in __init__\n",
      "    self._engine = self._make_engine(self.engine)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1050, in _make_engine\n",
      "    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1867, in __init__\n",
      "    self._open_handles(src, kwds)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1368, in _open_handles\n",
      "    storage_options=kwds.get(\"storage_options\", None),\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\", line 603, in get_handle\n",
      "    **compression_args,\n",
      "  File \"/opt/conda/lib/python3.7/bz2.py\", line 92, in __init__\n",
      "    self._fp = _builtin_open(filename, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/dumps/ru.tsv.bz2'\n",
      "08-20 09:52 : INFO : PROBABLY EXPECTED ERROR: No \"all-times\" file for ru\n",
      "08-20 09:56 : INFO : Loaded ru-2018 in 266.7513883113861\n",
      "08-20 10:01 : INFO : Loaded ru-2019 in 282.0229470729828\n",
      "08-20 10:06 : INFO : Loaded ru-2020 in 300.5508315563202\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "for code in codes:\n",
    "    df_dict[code] = process_lang_history(code, column_names, path=DUMPS_PATH, dtypes=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:11 : INFO : Created new singleton instance\n",
      "08-20 10:11 : INFO : User-Kind assignment for pl in 292.57357692718506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New logging instance for ../logging//pipeline.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "08-20 10:15 : INFO : Finished date assignment for pl in 522.6604981422424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b57e661db964035a6bab666a385a649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12279549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a56287b4374ab985bb69826fe0fc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/12279549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:16 : INFO : Finished name normalization for pl in 617.7120370864868\n",
      "08-20 10:18 : INFO : User-Kind assignment for cs in 103.80102610588074\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "08-20 10:19 : INFO : Finished date assignment for cs in 190.55896973609924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0698a760587044bcb25132560f130ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4357796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f960eecdb54fbe9a7ed79037b2be24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4357796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:20 : INFO : Finished name normalization for cs in 223.9407114982605\n",
      "08-20 10:25 : INFO : User-Kind assignment for uk in 287.56134510040283\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "08-20 10:29 : INFO : Finished date assignment for uk in 509.84636664390564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95101013f950450892659bc7dc9be210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11808705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b1d2a3f0374e5298a4706ff2274566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11808705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:30 : INFO : Finished name normalization for uk in 603.080760717392\n",
      "08-20 10:40 : INFO : User-Kind assignment for ru in 590.1830124855042\n",
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n",
      "08-20 10:48 : INFO : Finished date assignment for ru in 1082.308542251587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219450a90942443d8990ba7e6e7968ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25160718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bb13c39fc54b24898bf4c5f8882863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/25160718 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:52 : INFO : Finished name normalization for ru in 1300.5032720565796\n"
     ]
    }
   ],
   "source": [
    "Path(f'{PRE_PATH}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # === user kind merged to hear\n",
    "        df_dict[code]['user_kind'] = df_dict[code].apply(lambda row: 'anonymous' if pd.isna(row.event_user_id) else 'bot' if not pd.isna(row.event_user_is_bot_by) else 'account', axis=1)\n",
    "        Logger.instance('pipeline').info(f'User-Kind assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Convert timestamp\n",
    "        df_dict[code]['event_timestamp_t'] = df_dict[code].event_timestamp.swifter.apply(pd.to_datetime) #pd.to_datetime(df_dict[code].event_timestamp)\n",
    "        # Set UTC date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_localize(\"UTC\", ambiguous='NaT', nonexistent='NaT')\n",
    "        df_dict[code][\"date_utc\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        # Localize date\n",
    "        df_dict[code][\"event_timestamp_t\"] = df_dict[code].event_timestamp_t.dt.tz_convert(tz_[code])\n",
    "        df_dict[code][\"date\"] = df_dict[code].event_timestamp_t.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "        Logger.instance('pipeline').info(f'Finished date assignment for {code} in {time.time() - start}')\n",
    "        \n",
    "        # === Normalize date\n",
    "        df_dict[code]['page_title_norm'] = df_dict[code].page_title.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        df_dict[code]['page_title_historical_norm'] = df_dict[code].page_title_historical.swifter.apply(lambda title: mw_t.normalize(str(title)) if not pd.isna(title) else np.nan)\n",
    "        Logger.instance('pipeline').info(f'Finished name normalization for {code} in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL! save preprocessing so far\n",
    "# NOTE: takes up lots of memory.\n",
    "# write to processed\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_dict[code].to_csv(f'{PRE_PATH}/{code}_mwh_processed.tsv.gz', header=True, index=False, sep=\"\\t\", compression='gzip')\n",
    "        Logger.instance('pipeline').info(f'Dumping {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error when saving {code}: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and User_Kind to get newcomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.files import save_to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 10:53 : INFO : User creation computation for pl done in 56.310720682144165\n",
      "08-20 10:53 : INFO : User creation computation for cs done in 19.341925144195557\n",
      "08-20 10:54 : INFO : User creation computation for uk done in 49.41966986656189\n",
      "08-20 10:56 : INFO : User creation computation for ru done in 114.93536806106567\n",
      "08-20 10:56 : INFO : Finished newcomers\n"
     ]
    }
   ],
   "source": [
    "dict_creation = {}\n",
    "for code in codes:\n",
    "    start = time.time()\n",
    "    try:\n",
    "        df_code = df_dict[code]\n",
    "        # define masks\n",
    "        create_event_mask = (df_code.event_entity=='user') & (df_code.event_type == 'create')\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        no_bot_mask = (df_code['event_user_is_bot_by'].isna() | df_code['event_user_is_bot_by_historical'].isna())\n",
    "        self_creation_mask = (df_code['event_user_is_created_by_self'] == 'true')\n",
    "        no_anon_mask = (df_code['event_user_is_anonymous'] != 'true')\n",
    "\n",
    "        # === get users by registration\n",
    "        group_creation = df_code[create_event_mask & no_anon_mask & no_bot_mask & self_creation_mask].groupby(['date'])['event_user_id'].size()\n",
    "\n",
    "        # === get user by nth edit\n",
    "        # n=1\n",
    "        edit_count_mask = df_code.event_user_revision_count == '1'\n",
    "        group_edit1 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_1')\n",
    "        # n=5\n",
    "        edit_count_mask = df_code.event_user_revision_count == '5'\n",
    "        group_edit5 = df_code[create_revision_mask & no_anon_mask & no_bot_mask & edit_count_mask].groupby(['date'])['event_user_id'].size().rename('edit_5')\n",
    "        dict_creation[code] = pd.concat([group_creation, group_edit1, group_edit5], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'User creation computation for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')\n",
    "        \n",
    "save_to_pickle(f'{PRE_PATH}/dict_newcomers_selfcreated.pkl', dict_creation)\n",
    "Logger.instance('pipeline').info(f'Finished newcomers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and Page and user_kind to get edits per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:03 : INFO : Grouped by user/user_kind/pageid for pl done in 423.79040598869324\n",
      "08-20 11:05 : INFO : Grouped by user/user_kind/pageid for cs done in 136.60084629058838\n",
      "08-20 11:10 : INFO : Grouped by user/user_kind/pageid for uk done in 296.16267561912537\n",
      "08-20 11:21 : INFO : Grouped by user/user_kind/pageid for ru done in 686.6741018295288\n",
      "08-20 11:24 : INFO : Finished edits\n"
     ]
    }
   ],
   "source": [
    "# Group By Date and Page and user_kind to get edits per day\n",
    "dict_edits_byid = {} # grouped by id\n",
    "dict_edits_bytitle = {} # grouped by title\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # group by date, page_id, user_kind\n",
    "        df_code.revision_text_bytes_diff = pd.to_numeric(df_code['revision_text_bytes_diff'], errors='coerce').fillna(0)\n",
    "        df_code_masked = df_code[create_revision_mask & ns_mask]\n",
    "        \n",
    "        dict_edits_byid[code] = df_code_masked.groupby(['date', 'page_id', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_title': 'last', \n",
    "             'page_title_norm': 'last', 'page_title_historical_norm': 'last'})\n",
    "        dict_edits_bytitle[code] = df_code_masked.groupby(['date', 'page_title_norm', 'user_kind']).agg(\n",
    "            {'event_user_id': 'size', 'revision_text_bytes_diff': 'sum', 'page_id': lambda x: set(x), 'page_title': lambda x: set(x),  'page_title_historical_norm': lambda x: set(x)})\n",
    "\n",
    "        Logger.instance('pipeline').info(f'Grouped by user/user_kind/pageid for {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_byid.pkl', dict_edits_byid)\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle.pkl', dict_edits_bytitle)\n",
    "Logger.instance('pipeline').info(f'Finished edits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Date and user_kind to get identity reverts per day (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:24 : INFO : Computed reverts by pl done in 14.56476640701294\n",
      "08-20 11:24 : INFO : Computed reverts by cs done in 5.12779974937439\n",
      "08-20 11:24 : INFO : Computed reverts by uk done in 12.289405107498169\n",
      "08-20 11:25 : INFO : Computed reverts by ru done in 36.65907669067383\n",
      "08-20 11:25 : INFO : Finished identity reverts\n"
     ]
    }
   ],
   "source": [
    "dict_reverts = {}\n",
    "for code in codes:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_code = df_dict[code]\n",
    "        create_revision_mask = (df_code.event_entity=='revision') & (df_code.event_type == 'create')\n",
    "        ns_mask = df_code.page_namespace == '0'\n",
    "\n",
    "        # get reverts per day as well as reverted\n",
    "        df_reverted = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_reverted == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_reverted'].size()\n",
    "        df_reverts = df_code[create_revision_mask & ns_mask & (df_code.revision_is_identity_revert == 'true')].groupby(['date', 'user_kind'])['revision_is_identity_revert'].size()\n",
    "        \n",
    "        # reindex so all dates are filled\n",
    "        df_reverted = df_reverted.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverted.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "        df_reverts = df_reverts.reindex(\n",
    "            pd.MultiIndex.from_product([df_code.date.unique(), df_reverts.index.levels[1]], names=['date', 'user_kind']), fill_value=0)\n",
    "    \n",
    "        dict_reverts[code] = pd.concat([df_reverted, df_reverts], axis=1).fillna(0)\n",
    "        Logger.instance('pipeline').info(f'Computed reverts by {code} done in {time.time() - start}')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        Logger.instance('pipeline').info(f'Error for {code}: {str(e)}')  \n",
    "    \n",
    "save_to_pickle(f'{PRE_PATH}/dict_reverts.pkl', dict_reverts)\n",
    "Logger.instance('pipeline').info(f'Finished identity reverts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine edit dictionary with covid info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple demo, we exclude covid analysis. However, one could easily declare the articles they want to consider as COVID-19 articles (e.g., retrieving from https://covid-data.wmflabs.org/ or crawling COVID-categories via https://petscan.wmflabs.org/) and merge the result with the given dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file was pre-generated using the .json-list of COVID-articles from:\n",
    "# https://covid-data.wmflabs.org/\n",
    "'''\n",
    "path_covid = f'{DATA_PATH}/covid_linked.f'\n",
    "df_covid = pd.read_feather(path_covid)\n",
    "df_covid['covid'] = True\n",
    "df_covid['index'] = df_covid['index'].apply(lambda t: mw_t.normalize(str(t)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_edits_covid = {}\n",
    "for code, df_code in dict_edits_bytitle.items():\n",
    "    df_edits_covid[code] = df_code.reset_index().merge(df_covid[df_covid.site == f'{code}wiki'], left_on=['page_title_norm'], right_on=['index'], how='left').fillna({'covid': False}).drop(['index', 'site', 'qid'], axis=1)\n",
    "\n",
    "save_to_pickle(f'{PRE_PATH}/dict_edits_bytitle_covid.pkl', df_edits_covid)\n",
    "'''\n",
    "for code, df_code in dict_edits_bytitle.items():\n",
    "    df_code['covid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Final Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing import aggregate_preprocess_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:25 : INFO : Processing pl took 26.667002201080322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:25 : INFO : Processing cs took 8.161275863647461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:26 : INFO : Processing uk took 19.35340714454651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and making sure all dates are filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-20 11:26 : INFO : Processing ru took 43.44896745681763\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>covid</th>\n",
       "      <th>user_kind</th>\n",
       "      <th>count</th>\n",
       "      <th>rev_len_sum</th>\n",
       "      <th>actor_user</th>\n",
       "      <th>edit_1</th>\n",
       "      <th>edit_5</th>\n",
       "      <th>revision_is_identity_reverted</th>\n",
       "      <th>revision_is_identity_revert</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>4067</td>\n",
       "      <td>74545957.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>134</td>\n",
       "      <td>304</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>629</td>\n",
       "      <td>12513690.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>25</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>560</td>\n",
       "      <td>9184369.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>4264</td>\n",
       "      <td>77919376.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>143</td>\n",
       "      <td>314</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>824</td>\n",
       "      <td>14549224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331</td>\n",
       "      <td>18</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>2377</td>\n",
       "      <td>94509996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540</td>\n",
       "      <td>85</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3287</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>1254</td>\n",
       "      <td>28411553.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>account</td>\n",
       "      <td>1078</td>\n",
       "      <td>41496830.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>anonymous</td>\n",
       "      <td>206</td>\n",
       "      <td>10082671.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>False</td>\n",
       "      <td>bot</td>\n",
       "      <td>127</td>\n",
       "      <td>1812660.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13162 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  covid  user_kind  count  rev_len_sum  actor_user  edit_1  \\\n",
       "0    2018-01-01  False    account   4067   74545957.0        54.0    20.0   \n",
       "1    2018-01-01  False  anonymous    629   12513690.0         0.0     0.0   \n",
       "2    2018-01-01  False        bot    560    9184369.0         0.0     0.0   \n",
       "3    2018-01-02  False    account   4264   77919376.0       168.0    76.0   \n",
       "4    2018-01-02  False  anonymous    824   14549224.0         0.0     0.0   \n",
       "...         ...    ...        ...    ...          ...         ...     ...   \n",
       "3286 2020-12-31  False  anonymous   2377   94509996.0         0.0     0.0   \n",
       "3287 2020-12-31  False        bot   1254   28411553.0         0.0     0.0   \n",
       "3288 2021-01-01  False    account   1078   41496830.0        15.0     8.0   \n",
       "3289 2021-01-01  False  anonymous    206   10082671.0         0.0     0.0   \n",
       "3290 2021-01-01  False        bot    127    1812660.0         0.0     0.0   \n",
       "\n",
       "      edit_5  revision_is_identity_reverted  revision_is_identity_revert code  \n",
       "0       16.0                            134                          304   pl  \n",
       "1        0.0                            219                           25   pl  \n",
       "2        0.0                              5                            2   pl  \n",
       "3       25.0                            143                          314   pl  \n",
       "4        0.0                            331                           18   pl  \n",
       "...      ...                            ...                          ...  ...  \n",
       "3286     0.0                            540                           85   ru  \n",
       "3287     0.0                             20                           52   ru  \n",
       "3288     4.0                             40                           48   ru  \n",
       "3289     0.0                             65                            5   ru  \n",
       "3290     0.0                              0                           16   ru  \n",
       "\n",
       "[13162 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_agg = aggregate_preprocess_results(codes, df_edits_covid, dict_creation, dict_reverts)\n",
    "final_agg = aggregate_preprocess_results(codes, dict_edits_bytitle, dict_creation, dict_reverts)\n",
    "final_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{RES_PATH}/demo/').mkdir(parents=True, exist_ok=True)\n",
    "final_agg.to_csv(f'{RES_PATH}/demo/aggregated_demo.tsv.gz', index=False, sep=\"\\t\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}